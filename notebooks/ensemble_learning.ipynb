{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble learning\n",
    "Notebook to explore improvement in performance of building custom-made ensemble learners. The workflow is the following:\n",
    "1. Build-up of the individual learners and performance evaluation\n",
    "- LighGBM\n",
    "- XGBoost\n",
    "- Random forest\n",
    "- (Maybe) lasso regression\n",
    "In this part, we will also include the resampling of data performing upsampling + downsampling\n",
    "\n",
    "2. Study on how to ensemble them together for performance optimization\n",
    "- Hard voting (including predictive threshold performance optimization for all of them)\n",
    "- Soft voting (with a posterior predictive threshold optimization)\n",
    "- Stacking\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import importlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from xgboost import XGBClassifier, plot_importance\n",
    "from tqdm import tqdm\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from auxFuns.EDA import *\n",
    "from auxFuns.modelling import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets_path = os.getcwd() + '/datasets/raw'\n",
    "processed_datasets_path = os.getcwd() + '/datasets/processed'\n",
    "\n",
    "rsv_predictors_df_v2 = pd.read_csv(processed_datasets_path + '/rsv_predictors_phase1_daysDedup_seasons_prevTest.csv',low_memory=False)\n",
    "rsv_predictors_df_v2 = make_it_categorical_v2(rsv_predictors_df_v2)\n",
    "\n",
    "rsv_predictors_df_v2.shape\n",
    "\n",
    "# summary_function_rsv(rsv_predictors_df_v2)\n",
    "\n",
    "# Extract a reduced sample of the data for modelling\n",
    "sample_size = 80000\n",
    "sample_v2_df = rsv_predictors_df_v2.sample(n = sample_size, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['sex', 'marital_status', 'race','patient_regional_location', 'age_group',\n",
    "                     'Acute_upper_respiratory_infection','Influenza','Pneumonia','Bronchitis','Symptoms_and_signs__digestive_system_and_abdomen','General_symptoms_and_signs','any_symptom',\n",
    "                     'COPD','AIDS','Asthma_chronic','CCI',\n",
    "                     'sine','cosine','calendar_year', \n",
    "                     'healthcare_seeking', 'influenza_vaccine',\n",
    "                     'n_symptoms','prev_positive_rsv','previous_test_daydiff','n_immunodeficiencies', \n",
    "                     'tumor_indicator','tumor_last_year',\n",
    "                     'season']\n",
    "# selected_features = ['sex', 'marital_status', 'race', 'patient_regional_location', 'age_group',\n",
    "#                      'Acute_upper_respiratory_infection','Influenza','Pneumonia','Bronchitis','Symptoms_and_signs__digestive_system_and_abdomen','General_symptoms_and_signs','any_symptom',\n",
    "#                      'COPD','AIDS','Asthma_chronic','CCI',\n",
    "#                      'sine','cosine','calendar_year', \n",
    "#                      'healthcare_seeking', 'influenza_vaccine',\n",
    "#                      'n_symptoms','prev_positive_rsv','previous_test_daydiff','n_immunodeficiencies', \n",
    "#                      'tumor_indicator','tumor_last_year']\n",
    "selected_features.append('RSV_test_result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampling method chosen:\n",
      "\n",
      "Undersampling\n"
     ]
    }
   ],
   "source": [
    "df1 = sample_v2_df[selected_features]\n",
    "\n",
    "input_test_size = 0.2\n",
    "random_seed = 42\n",
    "\n",
    "X_train, y_train, X_test, y_test, preprocessor_rsv = preprocess_and_resample_rsv(\n",
    "    df1, input_test_size = input_test_size, random_seed = random_seed, resampling_technique = 'under', ratio_maj_min = 0.8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Study resampling techniques (WIP)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Build-up of the models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. XGboost\n",
    "- Train it using the previous approach (GridSearchCV)\n",
    "- Train it using Bayesian parameter optimization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Approach 1: GridSearch CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 1: GridSearch CV\n",
    "random_seed = 42\n",
    "cost_sensitive = True\n",
    "\n",
    "if cost_sensitive:\n",
    "    weight_dict = {\"Negative\": 1,\n",
    "                   \"Positive\": 50}\n",
    "    scale_pos_weight = weight_dict[\"Positive\"]/weight_dict[\"Negative\"]  # Use scale_pos_weight parameter\n",
    "    model_class = XGBClassifier(scale_pos_weight=scale_pos_weight,\n",
    "                                random_state=random_seed)\n",
    "else:\n",
    "    model_class = XGBClassifier(random_state=random_seed)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': range(20,205,15),\n",
    "    'max_depth': range(5,30,1),\n",
    "    'learning_rate': np.arange(0.01, 0.51, 0.05),\n",
    "    'min_child_weight': np.arange(1, 11, 1), \n",
    "    'gamma': np.arange(0.1, 0.5, 0.1) \n",
    "}\n",
    "\n",
    "target_scorer = make_scorer(f1_score, average='macro')\n",
    "n_cv_folds = 5\n",
    "\n",
    "# XGBoost needs labels in numeric format\n",
    "y_train_numeric = [1 if label == \"Positive\" else 0 for label in y_train]\n",
    "\n",
    "model1_xgb = train_model_rsv(model = model_class, param_grid = param_grid, target_scorer = target_scorer, n_cv_folds = n_cv_folds,\n",
    "                    X_train = X_train, y_train = y_train_numeric)\n",
    "\n",
    "optimal_threshold = find_optimal_moving_threshold(model = model1_xgb, X_test = X_test, y_test = y_test)\n",
    "__,__,__,__,__,__,f1 = calculate_performance_metrics_rsv(trained_model = model1_xgb, X_test = X_test, y_test = y_test,\n",
    "                                                         threshold = optimal_threshold, \n",
    "                                                         print_roc = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Approach 2: Bayesian hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "Training XGBoost classifier with objective metric: f1\n",
      "Tuning Hyperparameters ...\n"
     ]
    }
   ],
   "source": [
    "# Approach 2: train the model using bayesian hyperparameter optimization\n",
    "\n",
    "# Scorings = ['accuracy', 'balanced_accuracy', 'f1', 'f1_micro', 'f1_macro', 'f1_weighted', 'precision', 'recall', 'roc_auc']\n",
    "Scorings = ['f1', 'f1_micro', 'f1_macro', 'f1_weighted', 'recall', 'roc_auc']\n",
    "y_train_numeric = [1 if label == 'Positive' else 0 for label in y_train]\n",
    "\n",
    "best_models = {}\n",
    "\n",
    "for score in tqdm(Scorings):\n",
    "    classifier = XGBoostClassifier_custom(scoring=score, max_evals=12)\n",
    "    classifier.train(X_train, y_train_numeric)\n",
    "    classifier.predict(X_test, y_test)\n",
    "    best_models[score] = {'model': classifier.model, 'score_f1': classifier.score_f1, 'score_auc': classifier.score_auc}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBoostClassifier_custom:\n",
    "\n",
    "    def __init__(self, scoring, max_evals):\n",
    "        self.scoring = scoring\n",
    "        self.max_evals = max_evals\n",
    "        self.best = None\n",
    "        self.model = None\n",
    "        self.score_f1 = None\n",
    "        self.score_auc = None\n",
    "\n",
    "    def objective(self, space):\n",
    "        classifier = XGBClassifier(n_estimators = space['n_estimators'],\n",
    "                                    max_depth = int(space['max_depth']),\n",
    "                                    learning_rate = space['learning_rate'],\n",
    "                                    gamma = space['gamma'],\n",
    "                                    min_child_weight = space['min_child_weight'],\n",
    "                                    subsample = space['subsample'],\n",
    "                                    colsample_bytree = space['colsample_bytree'],\n",
    "                                    )\n",
    "        classifier.fit(self.X_train, self.y_train)\n",
    "        \n",
    "        Scores = cross_val_score(estimator = classifier, X = self.X_train, y = self.y_train, cv = 10, scoring=self.scoring)\n",
    "        score = Scores.mean()\n",
    "        loss = 1-score\n",
    "        return {'loss': loss, 'status': STATUS_OK}\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        print('--------------------------------------------------------------------')\n",
    "        print(f'Training XGBoost classifier with objective metric: {self.scoring}')\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "        space = {\n",
    "        'max_depth' : hp.choice('max_depth', [6,8,12]),\n",
    "        'learning_rate' : hp.choice('learning_rate', [0.001, 0.01]),\n",
    "        'n_estimators' : hp.choice('n_estimators', [1000, 5000]),\n",
    "        'gamma' : hp.quniform('gamma', 0, 0.50, 0.1),\n",
    "        'min_child_weight' : hp.quniform('min_child_weight', 1, 10, 2),\n",
    "        'subsample' : hp.quniform('subsample', 0.6, 1, 0.1),\n",
    "        'colsample_bytree' : hp.quniform('colsample_bytree', 0.6, 1.1, 0.1),\n",
    "        'early_stopping_rounds': hp.choice('early_stopping_rounds', [50, 100])\n",
    "        }\n",
    "\n",
    "        trials = Trials()\n",
    "        print(\"Tuning Hyperparameters ...\")\n",
    "        self.best = fmin(fn=self.objective,\n",
    "                    space=space,\n",
    "                    algo=tpe.suggest,\n",
    "                    max_evals=self.max_evals,\n",
    "                    trials=trials)\n",
    "        print(\"Best Hyperparameters: \", self.best)\n",
    "        self.fit_model()\n",
    "\n",
    "    def fit_model(self):\n",
    "        self.model = XGBClassifier(n_estimators = self.best['n_estimators'],\n",
    "                                max_depth = self.best['max_depth'],\n",
    "                                learning_rate = self.best['learning_rate'],\n",
    "                                gamma = self.best['gamma'],\n",
    "                                min_child_weight = self.best['min_child_weight'],\n",
    "                                subsample = self.best['subsample'],\n",
    "                                colsample_bytree = self.best['colsample_bytree'], \n",
    "                                early_stopping_rounds = self.best['early_stopping_rounds'],\n",
    "                                verbose = True\n",
    "                                )\n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "\n",
    "        print('XGBoostClassifier Performance:')\n",
    "\n",
    "        # Scores_f1 = cross_val_score(estimator = self.model, X = self.X_train, y = self.y_train, cv = 10, scoring='f1')\n",
    "        # self.score_f1 = Scores_f1.mean()\n",
    "        # print(\"Train Set 10-Fold F1-Score: \", self.score)\n",
    "\n",
    "        # Scores_auc = cross_val_score(estimator = self.model, X = self.X_train, y = self.y_train, cv = 10, scoring='roc_auc')\n",
    "        # self.score_auc = Scores_auc.mean()\n",
    "        # print(\"Train Set 10-Fold F1-Score: \", self.score)\n",
    "\n",
    "    def predict(self, X_test, y_test):\n",
    "\n",
    "        optimal_threshold = find_optimal_moving_threshold(model = self.model, X_test = X_test, y_test = y_test)\n",
    "        __,__,__,__,__,__,__ = calculate_performance_metrics_rsv(trained_model = self.model, X_test = X_test, y_test = y_test,\n",
    "                                                         threshold = optimal_threshold, \n",
    "                                                         print_roc = False)\n",
    "        # # F1 score - Test set\n",
    "        # self.y_pred = self.model.predict(X_test)\n",
    "        # score_test = f1_score(y_test, y_pred)\n",
    "        # print(\"Test Set F1-Score: \", score_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
