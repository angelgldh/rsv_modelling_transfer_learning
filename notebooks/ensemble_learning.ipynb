{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble learning\n",
    "Notebook to explore improvement in performance of building custom-made ensemble learners. The workflow is the following:\n",
    "1. Build-up of the individual learners and performance evaluation\n",
    "- LighGBM\n",
    "- XGBoost\n",
    "- Random forest\n",
    "- (Maybe) lasso regression\n",
    "In this part, we will also include the resampling of data performing upsampling + downsampling\n",
    "\n",
    "2. Study on how to ensemble them together for performance optimization\n",
    "- Hard voting (including predictive threshold performance optimization for all of them)\n",
    "- Soft voting (with a posterior predictive threshold optimization)\n",
    "- Stacking\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import importlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from xgboost import XGBClassifier, plot_importance\n",
    "from tqdm import tqdm\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import lightgbm as lgbm\n",
    "\n",
    "from auxFuns.EDA import *\n",
    "from auxFuns.modelling import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'auxFuns.modelling' from 'c:\\\\Users\\\\angel\\\\Documents\\\\VSCode\\\\rsv_modelling_transfer_learning\\\\auxFuns\\\\modelling.py'>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import auxFuns.modelling\n",
    "importlib.reload(auxFuns.modelling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets_path = os.getcwd() + '/datasets/raw'\n",
    "processed_datasets_path = os.getcwd() + '/datasets/processed'\n",
    "\n",
    "rsv_predictors_df_v2 = pd.read_csv(processed_datasets_path + '/rsv_predictors_phase1_daysDedup_seasons_prevTest.csv',low_memory=False)\n",
    "rsv_predictors_df_v2 = make_it_categorical_v2(rsv_predictors_df_v2)\n",
    "\n",
    "rsv_predictors_df_v2.shape\n",
    "\n",
    "# summary_function_rsv(rsv_predictors_df_v2)\n",
    "\n",
    "# Extract a reduced sample of the data for modelling\n",
    "sample_size = 80000\n",
    "sample_v2_df = rsv_predictors_df_v2.sample(n = sample_size, random_state=42)\n",
    "\n",
    "sample_v2_df = rsv_predictors_df_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['sex', 'marital_status', 'race','patient_regional_location', 'age_group',\n",
    "                     'Acute_upper_respiratory_infection','Influenza','Pneumonia','Bronchitis','Symptoms_and_signs__digestive_system_and_abdomen','General_symptoms_and_signs','any_symptom',\n",
    "                     'COPD','AIDS','Asthma_chronic','CCI',\n",
    "                     'sine','cosine','calendar_year', \n",
    "                     'healthcare_seeking', 'influenza_vaccine',\n",
    "                     'n_symptoms','prev_positive_rsv','previous_test_daydiff','n_immunodeficiencies', \n",
    "                     'tumor_indicator','tumor_last_year',\n",
    "                     'season',\n",
    "                     'n_tests_that_day']\n",
    "# selected_features = ['sex', 'marital_status', 'race', 'patient_regional_location', 'age_group',\n",
    "#                      'Acute_upper_respiratory_infection','Influenza','Pneumonia','Bronchitis','Symptoms_and_signs__digestive_system_and_abdomen','General_symptoms_and_signs','any_symptom',\n",
    "#                      'COPD','AIDS','Asthma_chronic','CCI',\n",
    "#                      'sine','cosine','calendar_year', \n",
    "#                      'healthcare_seeking', 'influenza_vaccine',\n",
    "#                      'n_symptoms','prev_positive_rsv','previous_test_daydiff','n_immunodeficiencies', \n",
    "#                      'tumor_indicator','tumor_last_year']\n",
    "selected_features.append('RSV_test_result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampling method chosen:\n",
      "\n",
      "Downsampling and Upweighting\n"
     ]
    }
   ],
   "source": [
    "df1 = sample_v2_df[selected_features]\n",
    "\n",
    "input_test_size = 0.2\n",
    "random_seed = 42\n",
    "\n",
    "X_train, y_train, X_test, y_test, sample_weights, preprocessor_rsv = preprocess_and_resample_rsv(\n",
    "    df1, input_test_size = input_test_size, random_seed = random_seed, resampling_technique = \"downsample_upweight\", ratio_maj_min = 0.7)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Study resampling techniques (WIP)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Build-up of the models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. XGboost\n",
    "- Train it using the previous approach (GridSearchCV)\n",
    "- Train it using Bayesian parameter optimization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Approach 1: GridSearch CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 1: GridSearch CV\n",
    "random_seed = 42\n",
    "cost_sensitive = True\n",
    "\n",
    "if cost_sensitive:\n",
    "    weight_dict = {\"Negative\": 1,\n",
    "                   \"Positive\": 50}\n",
    "    scale_pos_weight = weight_dict[\"Positive\"]/weight_dict[\"Negative\"]  # Use scale_pos_weight parameter\n",
    "    model_class = XGBClassifier(scale_pos_weight=scale_pos_weight,\n",
    "                                random_state=random_seed)\n",
    "else:\n",
    "    model_class = XGBClassifier(random_state=random_seed)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': range(20,205,15),\n",
    "    'max_depth': range(5,30,1),\n",
    "    'learning_rate': np.arange(0.01, 0.51, 0.05),\n",
    "    'min_child_weight': np.arange(1, 11, 1), \n",
    "    'gamma': np.arange(0.1, 0.5, 0.1) \n",
    "}\n",
    "\n",
    "target_scorer = make_scorer(f1_score, average='macro')\n",
    "n_cv_folds = 5\n",
    "\n",
    "# XGBoost needs labels in numeric format\n",
    "y_train_numeric = [1 if label == \"Positive\" else 0 for label in y_train]\n",
    "\n",
    "model1_xgb = train_model_rsv(model = model_class, param_grid = param_grid, target_scorer = target_scorer, n_cv_folds = n_cv_folds,\n",
    "                    X_train = X_train, y_train = y_train_numeric)\n",
    "\n",
    "optimal_threshold = find_optimal_moving_threshold(model = model1_xgb, X_test = X_test, y_test = y_test)\n",
    "__,__,__,__,__,__,f1 = calculate_performance_metrics_rsv(trained_model = model1_xgb, X_test = X_test, y_test = y_test,\n",
    "                                                         threshold = optimal_threshold, \n",
    "                                                         print_roc = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Approach 2: Bayesian hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "Training XGBoost classifier with objective metric: f1\n",
      "Tuning Hyperparameters ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:40<00:00,  8.37s/trial, best loss: 0.24443137577723262]\n",
      "Best Hyperparameters:  {'colsample_bytree': 0.7000000000000001, 'gamma': 0.0, 'learning_rate': 0.1, 'max_depth': 5.0, 'min_child_weight': 8.0, 'n_estimators': 5, 'subsample': 0.9}\n",
      "[18:44:51] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0fdc6d574b9c0d168-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "XGBoostClassifier Performance:\n",
      "Optimal threshold: 0.61\n",
      "Optimal f1: 0.25958702064896755\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [01:43<08:38, 103.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.7557835693275874\n",
      "Precision / Positive predictive value: 0.4808743169398907\n",
      "Specificity: 0.9938729442115447\n",
      "Recall / sensitivity: 0.17777777777777778\n",
      "Negative predictive value: 0.9742681924511601\n",
      "Accuracy: 0.968625\n",
      "F-1: 0.25958702064896755\n",
      "--------------------------------------------------------------------\n",
      "Training XGBoost classifier with objective metric: f1_micro\n",
      "Tuning Hyperparameters ...\n",
      "100%|██████████| 12/12 [01:06<00:00,  5.58s/trial, best loss: 0.28301860069870644]\n",
      "Best Hyperparameters:  {'colsample_bytree': 0.7000000000000001, 'gamma': 0.30000000000000004, 'learning_rate': 0.15000000000000002, 'max_depth': 5.0, 'min_child_weight': 2.0, 'n_estimators': 4, 'subsample': 0.7000000000000001}\n",
      "[18:46:01] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0fdc6d574b9c0d168-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "XGBoostClassifier Performance:\n",
      "Optimal threshold: 0.62\n",
      "Optimal f1: 0.24242424242424243\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [02:53<05:35, 83.88s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.7597480773553009\n",
      "Precision / Positive predictive value: 0.42424242424242425\n",
      "Specificity: 0.9926475330538536\n",
      "Recall / sensitivity: 0.1696969696969697\n",
      "Negative predictive value: 0.9739906340969497\n",
      "Accuracy: 0.9671875\n",
      "F-1: 0.24242424242424243\n",
      "--------------------------------------------------------------------\n",
      "Training XGBoost classifier with objective metric: f1_macro\n",
      "Tuning Hyperparameters ...\n",
      "100%|██████████| 12/12 [01:25<00:00,  7.10s/trial, best loss: 0.2947847848462437]\n",
      "Best Hyperparameters:  {'colsample_bytree': 0.6000000000000001, 'gamma': 0.0, 'learning_rate': 0.05, 'max_depth': 15.0, 'min_child_weight': 6.0, 'n_estimators': 7, 'subsample': 0.9}\n",
      "[18:47:29] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0fdc6d574b9c0d168-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "XGBoostClassifier Performance:\n",
      "Optimal threshold: 0.6\n",
      "Optimal f1: 0.1961961961961962\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [04:21<04:17, 85.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.7520191270981338\n",
      "Precision / Positive predictive value: 0.19444444444444445\n",
      "Specificity: 0.9738148984198646\n",
      "Recall / sensitivity: 0.19797979797979798\n",
      "Negative predictive value: 0.9743804852865255\n",
      "Accuracy: 0.9498125\n",
      "F-1: 0.1961961961961962\n",
      "--------------------------------------------------------------------\n",
      "Training XGBoost classifier with objective metric: f1_weighted\n",
      "Tuning Hyperparameters ...\n",
      "100%|██████████| 12/12 [01:06<00:00,  5.53s/trial, best loss: 0.2834832698253249]\n",
      "Best Hyperparameters:  {'colsample_bytree': 0.9, 'gamma': 0.0, 'learning_rate': 0.1, 'max_depth': 5.0, 'min_child_weight': 4.0, 'n_estimators': 4, 'subsample': 0.6000000000000001}\n",
      "[18:48:38] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0fdc6d574b9c0d168-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "XGBoostClassifier Performance:\n",
      "Optimal threshold: 0.59\n",
      "Optimal f1: 0.2545454545454545\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [05:30<02:38, 79.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.7531737888397031\n",
      "Precision / Positive predictive value: 0.2909090909090909\n",
      "Specificity: 0.982392776523702\n",
      "Recall / sensitivity: 0.22626262626262628\n",
      "Negative predictive value: 0.975472302273455\n",
      "Accuracy: 0.959\n",
      "F-1: 0.2545454545454545\n",
      "--------------------------------------------------------------------\n",
      "Training XGBoost classifier with objective metric: recall\n",
      "Tuning Hyperparameters ...\n",
      "100%|██████████| 12/12 [01:30<00:00,  7.50s/trial, best loss: 0.2256535201258819]\n",
      "Best Hyperparameters:  {'colsample_bytree': 0.7000000000000001, 'gamma': 0.2, 'learning_rate': 0.05, 'max_depth': 10.0, 'min_child_weight': 8.0, 'n_estimators': 4, 'subsample': 0.7000000000000001}\n",
      "[18:50:11] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0fdc6d574b9c0d168-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "XGBoostClassifier Performance:\n",
      "Optimal threshold: 0.59\n",
      "Optimal f1: 0.23168654173764905\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 5/6 [07:07<01:25, 85.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.7549689086935136\n",
      "Precision / Positive predictive value: 0.7391304347826086\n",
      "Specificity: 0.9984521122218639\n",
      "Recall / sensitivity: 0.13737373737373737\n",
      "Negative predictive value: 0.9731581594166457\n",
      "Accuracy: 0.9718125\n",
      "F-1: 0.23168654173764905\n",
      "--------------------------------------------------------------------\n",
      "Training XGBoost classifier with objective metric: roc_auc\n",
      "Tuning Hyperparameters ...\n",
      "100%|██████████| 12/12 [02:13<00:00, 11.13s/trial, best loss: 0.206069339948042]\n",
      "Best Hyperparameters:  {'colsample_bytree': 0.8, 'gamma': 0.30000000000000004, 'learning_rate': 0.1, 'max_depth': 10.0, 'min_child_weight': 2.0, 'n_estimators': 3, 'subsample': 0.9}\n",
      "[18:52:31] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0fdc6d574b9c0d168-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "XGBoostClassifier Performance:\n",
      "Optimal threshold: 0.61\n",
      "Optimal f1: 0.21403912543153047\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [09:28<00:00, 94.76s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.7409889934494901\n",
      "Precision / Positive predictive value: 0.24866310160427807\n",
      "Specificity: 0.98187681393099\n",
      "Recall / sensitivity: 0.18787878787878787\n",
      "Negative predictive value: 0.9742736464866248\n",
      "Accuracy: 0.9573125\n",
      "F-1: 0.21403912543153047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Approach 2: train the model using bayesian hyperparameter optimization\n",
    "\n",
    "# Scorings = ['accuracy', 'balanced_accuracy', 'f1', 'f1_micro', 'f1_macro', 'f1_weighted', 'precision', 'recall', 'roc_auc']\n",
    "Scorings = ['f1', 'f1_micro', 'f1_macro', 'f1_weighted', 'recall', 'roc_auc']\n",
    "y_train_numeric = [1 if label == 'Positive' else 0 for label in y_train]\n",
    "\n",
    "best_models = {}\n",
    "\n",
    "for score in tqdm(Scorings):\n",
    "    classifier = XGBoostClassifier_custom(scoring=score, max_evals=12, sample_weights = sample_weights, cost_sensitive_yn = True)\n",
    "    classifier.train(X_train, y_train_numeric)\n",
    "    classifier.predict(X_test, y_test)\n",
    "    best_models[score] = {'model': classifier.model, 'score_f1': classifier.score_f1, 'score_auc': classifier.score_auc}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBoostClassifier_custom:\n",
    "\n",
    "    def __init__(self, scoring, max_evals, cost_sensitive_yn, sample_weights):\n",
    "        self.scoring = scoring\n",
    "        self.max_evals = max_evals\n",
    "        self.cost_sensitive = cost_sensitive_yn\n",
    "        self.sample_weights = sample_weights\n",
    "        self.best = None\n",
    "        self.model = None\n",
    "        self.score_f1 = None\n",
    "        self.score_auc = None\n",
    "        \n",
    "\n",
    "    def objective(self, space):\n",
    "        classifier = XGBClassifier(n_estimators = space['n_estimators'],\n",
    "                                    max_depth = int(space['max_depth']),\n",
    "                                    learning_rate = space['learning_rate'],\n",
    "                                    gamma = space['gamma'],\n",
    "                                    min_child_weight = space['min_child_weight'],\n",
    "                                    subsample = space['subsample'],\n",
    "                                    colsample_bytree = space['colsample_bytree'],\n",
    "                                    )\n",
    "        # classifier.fit(self.X_train, self.y_train, early_stopping_rounds = space['early_stopping_rounds'], eval_metric = 'logloss',  eval_set=eval_set, verbose=True)\n",
    "        if self.cost_sensitive:\n",
    "            classifier.fit(self.X_train, self.y_train, sample_weight = self.sample_weights)\n",
    "        else: \n",
    "            classifier.fit(self.X_train, self.y_train)\n",
    "\n",
    "        Scores = cross_val_score(estimator = classifier, X = self.X_train, y = self.y_train, cv = 10, scoring=self.scoring)\n",
    "        score = Scores.mean()\n",
    "        loss = 1-score\n",
    "        return {'loss': loss, 'status': STATUS_OK}\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        print('--------------------------------------------------------------------')\n",
    "        print(f'Training XGBoost classifier with objective metric: {self.scoring}')\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "        self.space = {\n",
    "        'max_depth' : hp.quniform('max_depth',  5, 21, 5),\n",
    "        'learning_rate' : hp.quniform('learning_rate', 0.010, 0.200, 0.050),\n",
    "        'n_estimators' : hp.choice('n_estimators', range(20, 205, 25)),\n",
    "        'gamma' : hp.quniform('gamma', 0, 0.50, 0.1),\n",
    "        'min_child_weight' : hp.quniform('min_child_weight', 1, 10, 2),\n",
    "        'subsample' : hp.quniform('subsample', 0.6, 1, 0.1),\n",
    "        'colsample_bytree' : hp.quniform('colsample_bytree', 0.6, 1.0, 0.1),\n",
    "        'early_stopping_rounds': 100\n",
    "        }\n",
    "\n",
    "        trials = Trials()\n",
    "        print(\"Tuning Hyperparameters ...\")\n",
    "        self.best = fmin(fn=self.objective,\n",
    "                    space=self.space,\n",
    "                    algo=tpe.suggest,\n",
    "                    max_evals=self.max_evals,\n",
    "                    trials=trials)\n",
    "        print(\"Best Hyperparameters: \", self.best)\n",
    "        self.fit_model()\n",
    "\n",
    "    def fit_model(self):\n",
    "        self.model = XGBClassifier(n_estimators = self.best['n_estimators'],\n",
    "                                max_depth = int(self.best['max_depth']),\n",
    "                                learning_rate = self.best['learning_rate'],\n",
    "                                gamma = self.best['gamma'],\n",
    "                                min_child_weight = self.best['min_child_weight'],\n",
    "                                subsample = self.best['subsample'],\n",
    "                                colsample_bytree = self.best['colsample_bytree'], \n",
    "                                verbose = True\n",
    "                                )\n",
    "        # self.model.fit(self.X_train, self.y_train, early_stopping_rounds = self.space['early_stopping_rounds'], eval_metric = 'logloss')\n",
    "        if self.cost_sensitive:\n",
    "            self.model.fit(self.X_train, self.y_train, sample_weight = self.sample_weights)\n",
    "        else: \n",
    "            self.model.fit(self.X_train, self.y_train)\n",
    "\n",
    "        print('XGBoostClassifier Performance:')\n",
    "\n",
    "        # Scores_f1 = cross_val_score(estimator = self.model, X = self.X_train, y = self.y_train, cv = 10, scoring='f1')\n",
    "        # self.score_f1 = Scores_f1.mean()\n",
    "        # print(\"Train Set 10-Fold F1-Score: \", self.score)\n",
    "\n",
    "        # Scores_auc = cross_val_score(estimator = self.model, X = self.X_train, y = self.y_train, cv = 10, scoring='roc_auc')\n",
    "        # self.score_auc = Scores_auc.mean()\n",
    "        # print(\"Train Set 10-Fold F1-Score: \", self.score)\n",
    "\n",
    "    def predict(self, X_test, y_test):\n",
    "\n",
    "        optimal_threshold = find_optimal_moving_threshold(model = self.model, X_test = X_test, y_test = y_test)\n",
    "        self.score_auc,__,__,__,__,__,self.score_f1 = calculate_performance_metrics_rsv(trained_model = self.model, X_test = X_test, y_test = y_test,\n",
    "                                                         threshold = optimal_threshold, \n",
    "                                                         print_roc = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. LightGBM\n",
    "\n",
    "As the Bayesian hyperparameter has proven more effective, this approach will be followed here too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightGBMClassifier_custom:\n",
    "\n",
    "    def __init__(self, scoring, max_evals, cost_sensitive_yn, sample_weights):\n",
    "        self.scoring = scoring\n",
    "        self.max_evals = max_evals\n",
    "        self.cost_sensitive = cost_sensitive_yn\n",
    "        self.sample_weights = sample_weights\n",
    "        self.best = None\n",
    "        self.model = None\n",
    "        self.score_f1 = None\n",
    "        self.score_auc = None\n",
    "        \n",
    "    def objective(self, space):\n",
    "        classifier = lgbm.LGBMClassifier(n_estimators = space['n_estimators'],\n",
    "                                    max_depth = int(space['max_depth']),\n",
    "                                    learning_rate = space['learning_rate'],\n",
    "                                    min_child_weight = space['min_child_weight'],\n",
    "                                    subsample = space['subsample'],\n",
    "                                    colsample_bytree = space['colsample_bytree'],\n",
    "                                    )\n",
    "        \n",
    "        if self.cost_sensitive:\n",
    "            classifier.fit(self.X_train, self.y_train, sample_weight = self.sample_weights)\n",
    "        else: \n",
    "            classifier.fit(self.X_train, self.y_train)\n",
    "\n",
    "        Scores = cross_val_score(estimator = classifier, X = self.X_train, y = self.y_train, cv = 10, scoring=self.scoring)\n",
    "        score = Scores.mean()\n",
    "        loss = 1-score\n",
    "        return {'loss': loss, 'status': STATUS_OK}\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        print('--------------------------------------------------------------------')\n",
    "        print(f'Training LightGBM classifier with objective metric: {self.scoring}')\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "        self.space = {\n",
    "        'max_depth' : hp.quniform('max_depth', 5, 21, 5),\n",
    "        'learning_rate' : hp.uniform('learning_rate', 0.010, 0.200),\n",
    "        'n_estimators' : hp.choice('n_estimators', range(20, 205, 25)),\n",
    "        'min_child_weight' : hp.quniform('min_child_weight', 1, 10, 2),\n",
    "        'subsample' : hp.quniform('subsample', 0.6, 1, 0.1),\n",
    "        'colsample_bytree' : hp.quniform('colsample_bytree', 0.6, 1.0, 0.1),\n",
    "        }\n",
    "\n",
    "        trials = Trials()\n",
    "        print(\"Tuning Hyperparameters ...\")\n",
    "        self.best = fmin(fn=self.objective,\n",
    "                    space=self.space,\n",
    "                    algo=tpe.suggest,\n",
    "                    max_evals=self.max_evals,\n",
    "                    trials=trials)\n",
    "        print(\"Best Hyperparameters: \", self.best)\n",
    "        self.fit_model()\n",
    "\n",
    "    def fit_model(self):\n",
    "        self.model = lgbm.LGBMClassifier(n_estimators = self.best['n_estimators'],\n",
    "                                max_depth = int(self.best['max_depth']),\n",
    "                                learning_rate = self.best['learning_rate'],\n",
    "                                min_child_weight = self.best['min_child_weight'],\n",
    "                                subsample = self.best['subsample'],\n",
    "                                colsample_bytree = self.best['colsample_bytree']\n",
    "                                )\n",
    "\n",
    "        if self.cost_sensitive:\n",
    "            self.model.fit(self.X_train, self.y_train, sample_weight = self.sample_weights)\n",
    "        else: \n",
    "            self.model.fit(self.X_train, self.y_train)\n",
    "\n",
    "        print('LightGBMClassifier Performance:')\n",
    "        # These should be part of the evaluation set, not the training set\n",
    "\n",
    "        # Scores_f1 = cross_val_score(estimator = self.model, X = self.X_train, y = self.y_train, cv = 10, scoring='f1')\n",
    "        # self.score_f1 = Scores_f1.mean()\n",
    "        # print(\"Train Set 10-Fold F1-Score: \", self.score_f1)\n",
    "\n",
    "        # Scores_auc = cross_val_score(estimator = self.model, X = self.X_train, y = self.y_train, cv = 10, scoring='roc_auc')\n",
    "        # self.score_auc = Scores_auc.mean()\n",
    "        # print(\"Train Set 10-Fold AUC Score: \", self.score_auc)\n",
    "\n",
    "    def predict(self, X_test, y_test):       \n",
    "        optimal_threshold = find_optimal_moving_threshold(model = self.model, X_test = X_test, y_test = y_test)\n",
    "        self.score_auc,__,__,__,__,__,self.score_f1 = calculate_performance_metrics_rsv(trained_model = self.model, X_test = X_test, y_test = y_test, threshold = optimal_threshold, print_roc = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "Training LightGBM classifier with objective metric: f1\n",
      "Tuning Hyperparameters ...\n",
      "100%|██████████| 12/12 [00:52<00:00,  4.35s/trial, best loss: 0.21952563377577106]\n",
      "Best Hyperparameters:  {'colsample_bytree': 0.8, 'learning_rate': 0.07340009969395415, 'max_depth': 15.0, 'min_child_weight': 2.0, 'n_estimators': 2, 'subsample': 0.9}\n",
      "LightGBMClassifier Performance:\n",
      "Optimal threshold: 0.56\n",
      "Optimal f1: 0.35348837209302325\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [00:58<04:54, 58.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.7702883429854845\n",
      "Precision / Positive predictive value: 1.0\n",
      "Specificity: 1.0\n",
      "Recall / sensitivity: 0.21468926553672316\n",
      "Negative predictive value: 0.9756111825944555\n",
      "Accuracy: 0.9757727167092726\n",
      "F-1: 0.35348837209302325\n",
      "--------------------------------------------------------------------\n",
      "Training LightGBM classifier with objective metric: f1_micro\n",
      "Tuning Hyperparameters ...\n",
      "100%|██████████| 12/12 [00:46<00:00,  3.90s/trial, best loss: 0.2704347491535858]\n",
      "Best Hyperparameters:  {'colsample_bytree': 0.8, 'learning_rate': 0.1174101408036662, 'max_depth': 5.0, 'min_child_weight': 6.0, 'n_estimators': 3, 'subsample': 0.9}\n",
      "LightGBMClassifier Performance:\n",
      "Optimal threshold: 0.5700000000000001\n",
      "Optimal f1: 0.3764705882352941\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [01:51<03:40, 55.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.7874705719183197\n",
      "Precision / Positive predictive value: 0.8590604026845637\n",
      "Specificity: 0.9987410826689047\n",
      "Recall / sensitivity: 0.24105461393596986\n",
      "Negative predictive value: 0.9763816444939343\n",
      "Accuracy: 0.9753660237043923\n",
      "F-1: 0.3764705882352941\n",
      "--------------------------------------------------------------------\n",
      "Training LightGBM classifier with objective metric: f1_macro\n",
      "Tuning Hyperparameters ...\n",
      "100%|██████████| 12/12 [00:44<00:00,  3.69s/trial, best loss: 0.27737213997397225]\n",
      "Best Hyperparameters:  {'colsample_bytree': 0.7000000000000001, 'learning_rate': 0.08237521488037088, 'max_depth': 5.0, 'min_child_weight': 2.0, 'n_estimators': 4, 'subsample': 0.8}\n",
      "LightGBMClassifier Performance:\n",
      "Optimal threshold: 0.5700000000000001\n",
      "Optimal f1: 0.3567073170731707\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [02:42<02:39, 53.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.7679194762560695\n",
      "Precision / Positive predictive value: 0.936\n",
      "Specificity: 0.999520412445297\n",
      "Recall / sensitivity: 0.22033898305084745\n",
      "Negative predictive value: 0.9757710540176743\n",
      "Accuracy: 0.9754822217057867\n",
      "F-1: 0.3567073170731707\n",
      "--------------------------------------------------------------------\n",
      "Training LightGBM classifier with objective metric: f1_weighted\n",
      "Tuning Hyperparameters ...\n",
      "100%|██████████| 12/12 [00:42<00:00,  3.58s/trial, best loss: 0.27641801022933987]\n",
      "Best Hyperparameters:  {'colsample_bytree': 1.0, 'learning_rate': 0.08467153389469056, 'max_depth': 20.0, 'min_child_weight': 4.0, 'n_estimators': 5, 'subsample': 0.6000000000000001}\n",
      "LightGBMClassifier Performance:\n",
      "Optimal threshold: 0.66\n",
      "Optimal f1: 0.35348837209302325\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [03:31<01:42, 51.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.792338532364991\n",
      "Precision / Positive predictive value: 1.0\n",
      "Specificity: 1.0\n",
      "Recall / sensitivity: 0.21468926553672316\n",
      "Negative predictive value: 0.9756111825944555\n",
      "Accuracy: 0.9757727167092726\n",
      "F-1: 0.35348837209302325\n",
      "--------------------------------------------------------------------\n",
      "Training LightGBM classifier with objective metric: recall\n",
      "Tuning Hyperparameters ...\n",
      "100%|██████████| 12/12 [00:40<00:00,  3.41s/trial, best loss: 0.15591947913898474]\n",
      "Best Hyperparameters:  {'colsample_bytree': 0.8, 'learning_rate': 0.03858693138737751, 'max_depth': 20.0, 'min_child_weight': 4.0, 'n_estimators': 1, 'subsample': 0.9}\n",
      "LightGBMClassifier Performance:\n",
      "Optimal threshold: 0.52\n",
      "Optimal f1: 0.35348837209302325\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 5/6 [04:17<00:49, 49.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.7533068453785112\n",
      "Precision / Positive predictive value: 1.0\n",
      "Specificity: 1.0\n",
      "Recall / sensitivity: 0.21468926553672316\n",
      "Negative predictive value: 0.9756111825944555\n",
      "Accuracy: 0.9757727167092726\n",
      "F-1: 0.35348837209302325\n",
      "--------------------------------------------------------------------\n",
      "Training LightGBM classifier with objective metric: roc_auc\n",
      "Tuning Hyperparameters ...\n",
      "100%|██████████| 12/12 [00:51<00:00,  4.30s/trial, best loss: 0.1948260836524922]\n",
      "Best Hyperparameters:  {'colsample_bytree': 0.8, 'learning_rate': 0.13886733343361768, 'max_depth': 5.0, 'min_child_weight': 4.0, 'n_estimators': 3, 'subsample': 0.7000000000000001}\n",
      "LightGBMClassifier Performance:\n",
      "Optimal threshold: 0.6\n",
      "Optimal f1: 0.3708206686930091\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [05:15<00:00, 52.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.7871131391974653\n",
      "Precision / Positive predictive value: 0.9606299212598425\n",
      "Specificity: 0.9997002577783106\n",
      "Recall / sensitivity: 0.2297551789077213\n",
      "Negative predictive value: 0.976060872110038\n",
      "Accuracy: 0.9759470137113642\n",
      "F-1: 0.3708206686930091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Scorings = ['accuracy', 'balanced_accuracy', 'f1', 'f1_micro', 'f1_macro', 'f1_weighted', 'precision', 'recall', 'roc_auc']\n",
    "Scorings = ['f1', 'f1_micro', 'f1_macro', 'f1_weighted', 'recall', 'roc_auc']\n",
    "y_train_numeric = [1 if label == 'Positive' else 0 for label in y_train]\n",
    "\n",
    "best_models = {}\n",
    "\n",
    "for score in tqdm(Scorings):\n",
    "    classifier = LightGBMClassifier_custom(scoring=score, max_evals=12, sample_weights = sample_weights, cost_sensitive_yn = True)\n",
    "    classifier.train(X_train, y_train_numeric)\n",
    "    classifier.predict(X_test, y_test)\n",
    "    best_models[score] = {'model': classifier.model, 'score_f1': classifier.score_f1, 'score_auc': classifier.score_auc}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ensemble of the models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MetaClassifier(Algorithms, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Function to build a soft-voting ensemble meta-classifier\n",
    "    Arguments:\n",
    "    Algorithms -- list of model iterations outputs of different algorithms\n",
    "    train_dict -- a dictionary of train set features and labels\n",
    "    test_dict (optional) -- a dictionary that contains test set features and labels\n",
    "    Return:\n",
    "    out -- a dictionary of meta-classifier model ('BestModel'), a dataframe of performance metrics of all models (Metrics), \n",
    "           and a dataframe of metaclassifier predicted labels and probabilities on trainset instances (train_df_pred)\n",
    "    \"\"\"\n",
    "\n",
    "    estimators=[]\n",
    "    weights = []\n",
    "    results = []\n",
    "\n",
    "    train_dict = {\n",
    "        'X_train': X_train,\n",
    "        'y_train': y_train \n",
    "    }\n",
    "    test_dict = {\n",
    "        'X_test ': X_test,\n",
    "        'y_test': y_test\n",
    "    }\n",
    "\n",
    "\n",
    "    print('Generating a Soft-Voting Ensemble Classifier...')\n",
    "\n",
    "    for alg in tqdm(Algorithms):\n",
    "        for metric in tqdm(list(alg.keys())):\n",
    "            # for voting ensemble\n",
    "            estimators.append((str(alg)+'_'+metric, alg[metric]['model']))\n",
    "            weights.append(alg[metric]['score_f1'])\n",
    "\n",
    "            # for results\n",
    "            results.append(eval_model(model=alg[metric]['model'], train_dict=train_dict, test_dict=test_dict))\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    results_df.sort_values(by='f1', ascending=False)\n",
    "\n",
    "    # keeping top five iterations for the ensemble\n",
    "    KeepIdx = results_df.nlargest(5, 'f1').index\n",
    "\n",
    "    BestEstimators= [estimators[i] for i in KeepIdx.tolist()]\n",
    "    BestWeights = [weights[i] for i in KeepIdx.tolist()]\n",
    "\n",
    "    X_train, y_train = train_dict['X_train'], train_dict['y_train']\n",
    "\n",
    "    MetaClassifier = VotingClassifier(estimators=BestEstimators, voting='soft', weights=BestWeights)\n",
    "    MetaClassifier = MetaClassifier.fit(X_train, y_train)\n",
    "\n",
    "    results_df = results_df.append(eval_model(MetaClassifier, train_dict, test_dict), ignore_index=True).sort_values(by='train_CV_f1', ascending=False) # add metaclassification model metrics\n",
    "\n",
    "    # storing metaclassifier performance on trainset\n",
    "    y_pred = MetaClassifier.predict(X_train)\n",
    "    y_proba =  MetaClassifier.predict_proba(X_train)\n",
    "    train_df_pred = X_train.copy()\n",
    "    train_df_pred['y_true'] = y_train\n",
    "    train_df_pred['y_hat'] = y_pred\n",
    "    train_df_pred['y_hat_proba'] = y_proba[:,1] # probability of belonging to class 1\n",
    "\n",
    "    out = {'BestModel': BestEstimators[0][1], 'Metrics':results_df, 'train_df_pred':train_df_pred}\n",
    "    return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
