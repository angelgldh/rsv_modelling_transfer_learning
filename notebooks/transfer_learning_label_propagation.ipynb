{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label propagation\n",
    "\n",
    "Goal: determine the number of non-diagnosed / non-tested individuals in the phase 2 population. To extend the knowledge on this unlabelled set from the small labelled subset, we will establish the followin setting:\n",
    "\n",
    "1. Baseline model > just run the validated transfer-learning-model (validated on phase 2 labelled data) on the unlabelled data\n",
    "2. A dual self-supervised learning (SSL) and Semi-supervised learning (Semi-SL) [1] will be implemented.\n",
    "3. Classic approach on label propagation [2]\n",
    "\n",
    "An open question needed to be answered to success in this task:\n",
    "- Shall we use labelled data from both phase 1 and phase 2, or exclusively phase 1 data?\n",
    "\n",
    "[1]: Yoon, J., Zhang, Y., Jordon, J., & van der Schaar, M. (2020). Vime: Extending the success of self-and semi-supervised learning to tabular domain. Advances in Neural Information Processing Systems, 33, 11033-11043.\n",
    "[2]: ZhuЃ, X., & GhahramaniЃн, Z. (2002). Learning from labeled and unlabeled data with label propagation. ProQuest Number: INFORMATION TO ALL USERS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading of libaries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import importlib\n",
    "import joblib\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# import from custom package\n",
    "from auxFuns.EDA import *\n",
    "from auxFuns.modelling import *\n",
    "from auxFuns.class_overlap import *\n",
    "from auxFuns.transfer_learning import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'auxFuns.transfer_learning' from 'c:\\\\Users\\\\angel\\\\Documents\\\\VSCode\\\\rsv_modelling_transfer_learning\\\\auxFuns\\\\transfer_learning.py'>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import auxFuns.EDA \n",
    "importlib.reload(auxFuns.EDA)\n",
    "\n",
    "import auxFuns.modelling\n",
    "importlib.reload(auxFuns.modelling)\n",
    "\n",
    "import auxFuns.class_overlap\n",
    "importlib.reload(auxFuns.class_overlap)\n",
    "\n",
    "import auxFuns.transfer_learning\n",
    "importlib.reload(auxFuns.transfer_learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((86058, 64), (291938, 63), (2867, 64))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load of the data and filter in the needed features\n",
    "\n",
    "raw_datasets_path = os.getcwd() + '/datasets/raw'\n",
    "processed_datasets_path = os.getcwd() + '/datasets/processed'\n",
    "\n",
    "# Phase 1 data\n",
    "rsv_predictors_df_v2 = pd.read_csv(processed_datasets_path + '/rsv_predictors_phase1_daysDedup_seasons_prevTest_v2.csv',low_memory=False)\n",
    "rsv_predictors_phase1_df = make_it_categorical_v2(rsv_predictors_df_v2)\n",
    "\n",
    "# Phase 2 data\n",
    "rsv_phase2_df = pd.read_csv(processed_datasets_path + '/rsv_phase2_all_features.csv',low_memory=False)\n",
    "rsv_phase2_df = make_it_categorical_v2(rsv_phase2_df, is_phase1 = False)\n",
    "\n",
    "# Small subset of labelled data of phase 2:\n",
    "labels_phase2_df = pd.read_csv(raw_datasets_path + '/rsv_test_phase2.csv',low_memory=False)\n",
    "labels_phase2_df = labels_phase2_df.rename(columns = {'RSV_test_date':'index_date'})\n",
    "\n",
    "# Compatibility issue detected: merging labelled and unlabelled data is challening due to incompatible date columns\n",
    "rsv_phase2_df.index_date = pd.to_datetime(rsv_phase2_df.index_date)\n",
    "labels_phase2_df.index_date = pd.to_datetime(labels_phase2_df.index_date)\n",
    "\n",
    "rsv_phase2_labelled_df = labels_phase2_df.merge(rsv_phase2_df, how='left', on=['patient_id', 'index_date'])\n",
    "# rsv_phase2_labelled_df = labels_phase2_df.merge(rsv_phase2_df, how='left', on=['patient_id'])\n",
    "rsv_phase2_labelled_df = rsv_phase2_labelled_df.dropna()\n",
    "\n",
    "rsv_predictors_phase1_df.shape, rsv_phase2_df.shape, rsv_phase2_labelled_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['sex', 'marital_status', 'race','patient_regional_location', 'age_group',\n",
    "                     'Acute_upper_respiratory_infection','Influenza','Pneumonia','Bronchitis','Symptoms_and_signs__digestive_system_and_abdomen','General_symptoms_and_signs','any_symptom',\n",
    "                     'COPD','AIDS','Asthma_chronic','CCI',\n",
    "                     'sine','cosine','calendar_year', \n",
    "                     'healthcare_seeking', 'influenza_vaccine',\n",
    "                     'n_symptoms','prev_positive_rsv','previous_test_daydiff','n_immunodeficiencies', \n",
    "                     'tumor_indicator','tumor_last_year',\n",
    "                     'season',\n",
    "                     'n_tests_that_day']\n",
    "selected_features_v1 = ['n_tests_that_day', 'sine','cosine', 'previous_test_daydiff',\n",
    "                     'Bronchitis', 'CCI',\n",
    "                     'Acute_upper_respiratory_infection', 'n_immunodeficiencies', 'n_symptoms',\n",
    "                     'healthcare_seeking', \n",
    "                     'General_symptoms_and_signs', 'prev_positive_rsv', 'Influenza',\n",
    "                     'season','multiple_tests']\n",
    "selected_features_v2 = ['n_tests_that_day', 'sine','cosine', 'previous_test_daydiff',\n",
    "                     'Bronchitis', 'CCI',\n",
    "                     'Acute_upper_respiratory_infection', 'n_immunodeficiencies', 'n_symptoms',\n",
    "                     'healthcare_seeking', \n",
    "                     'General_symptoms_and_signs', 'prev_positive_rsv', 'Influenza',\n",
    "                     'key_comorbidities','Pneumonia',\n",
    "                     'season','month_of_the_test','multiple_tests',\n",
    "                     'BPA','BPAI']\n",
    "selected_features_v3 = selected_features_v2 + ['race', 'age_group','marital_status','sex',\n",
    "                                                    'patient_regional_location','calendar_year']\n",
    "\n",
    "rsv_test_related_features = ['n_tests_that_day', 'previous_test_daydiff', 'multiple_tests']\n",
    "selected_features_v4 = selected_features_v2.copy()\n",
    "[selected_features_v4.remove(feature) for feature in rsv_test_related_features]\n",
    "\n",
    "selected_features_v4_aux = selected_features_v4.copy()\n",
    "selected_features_v1.append('RSV_test_result')\n",
    "selected_features_v2.append('RSV_test_result')\n",
    "selected_features_v3.append('RSV_test_result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((86058, 17), (2867, 17), (289071, 17))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_modelling_phase1 = rsv_predictors_phase1_df[selected_features_v4]\n",
    "df_modelling_phase2 = rsv_phase2_labelled_df[selected_features_v4]\n",
    "df_modelling_all_phase2 = rsv_phase2_df[selected_features_v4_aux]\n",
    "\n",
    "# differentiate between labelled and non-labelled data in phase 2\n",
    "labelled_data_phase2_mask = rsv_phase2_labelled_df.index\n",
    "labelled_mask = rsv_phase2_df.index.isin(labelled_data_phase2_mask)\n",
    "non_labelled_mask = ~rsv_phase2_df.index.isin(labelled_data_phase2_mask)\n",
    "\n",
    "df_phase2_labelled = df_modelling_phase2.copy()\n",
    "df_phase2_unlabelled = df_modelling_all_phase2.loc[non_labelled_mask,:]\n",
    "\n",
    "df_modelling_phase1.shape, df_phase2_labelled.shape, df_phase2_unlabelled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label propagation: classical / graph approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSL and Semi-SL for label propagation\n",
    "\n",
    "Guideline:\n",
    "\n",
    "**1. Self-supervised learning**\n",
    "\n",
    "(SSL: produce 'pseudolabels' on the purely unlabelled data)\n",
    "\n",
    "**Goal of 1**: learn relevant representations of the unlabelled data, transferable to the semi-supervised learning phase\n",
    "\n",
    "1.1 Implementing the Pretext Tasks:\n",
    "- Mask Vector Generation: mask some of the features\n",
    "- Sample corruption: create corrupted/noisy samples *^x^*\n",
    "\n",
    "1.2. Encoder and pretect predictive models:\n",
    "\n",
    "- encoder: to map corrputed samples *^x^* to a latent representation *z*\n",
    "- mask vector estimator: predicts which features of *^x^* had been masked (binary cross entropy loss)\n",
    "- feature vector estimator: predicts original sample *x* from *^x^* (autoencoder-like loss function)\n",
    "\n",
    "\n",
    "**2. Semi-supervised learning**\n",
    "\n",
    "**Goal of 2**: extend the prediction model on the unlabelled data, taking the small labelled subset as well as the representations provided by the autoencoder.\n",
    "\n",
    "2.1. Use of the encoder\n",
    "\n",
    "- To map the samples to a common latent space *Z*\n",
    "\n",
    "2.2. Training of the predictive model\n",
    "\n",
    "- Training on labelled data on the **supervised loss Ls** and on unlabelled data using **consisteny loss Lu**\n",
    "- Consitency loss Lu: for a sample *x*, build its corrupted counterpart *^x^* K times. Lu measures how much predictions for the original sample and the corrupted counterparts differ.\n",
    "\n",
    "\n",
    "The training step will involve both the labelled and unlabelled sets (unlabelled for the self-supervised, both for the semi-supervised)\n",
    "The evaluation of the model needs to be made on a held-out validation set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-supervised learning: building the i) encoder, ii) mask and iii) feature estimators\n",
    "\n",
    "Reminder: all of this part is done with UNLABELLED data only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask_vector(n_features, pm):\n",
    "    \"\"\"\n",
    "    Generates a binary mask vector using a Bernoulli distribution\n",
    "\n",
    "    Parameters: \n",
    "    n_features (int): number of features of the vector to be masked\n",
    "    pm (float): parameter of the masking (Bernoulli) distribution\n",
    "\n",
    "    Returns:\n",
    "    mask (np.ndarray): masking vector of the input feature, of length \n",
    "\n",
    "    \"\"\"\n",
    "    mask = np.random.binomial(1, pm, n_features)\n",
    "    return mask\n",
    "\n",
    "def generate_masked_sample(x, mask, data):\n",
    "    \"\"\"\n",
    "    Generates a corrupted version of the sample using the mask vector\n",
    "\n",
    "    Given a sample `x` and a binary mask vector, this function returns a version of `x` where the \n",
    "    elements corresponding to a '1' in the mask vector are replaced with random samples from `data`.\n",
    "    If the mask vector contains '0' for an index, the original value from `x` is retained.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    x : numpy.array\n",
    "        A 1D numpy array representing the sample to be corrupted.\n",
    "        Its length should match the length of the `mask` vector.\n",
    "\n",
    "    mask : numpy.array\n",
    "        A 1D binary numpy array where '1' indicates the positions in `x` to be corrupted, and '0' \n",
    "        indicates the positions to be left unchanged. Should be of same length as `x`.\n",
    "\n",
    "    data : numpy.array or list\n",
    "        A collection from which random values are drawn to replace the corrupted positions in `x`. \n",
    "        It can be a list or a 1D numpy array.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    numpy.array\n",
    "        A corrupted version of `x` where values have been replaced based on the mask vector.\n",
    "\n",
    "    Examples:\n",
    "    --------\n",
    "    >>> x = np.array([1, 2, 3, 4, 5])\n",
    "    >>> mask = np.array([0, 1, 0, 1, 0])\n",
    "    >>> data = np.array([10, 11, 12, 13, 14, 15])\n",
    "    >>> generate_masked_sample(x, mask, data)\n",
    "    array([ 1, 11,  3, 13,  5])\n",
    "    \"\"\"\n",
    "    flattened_data = data.flatten()  # Flatten the data\n",
    "    x_corrupted = np.where(mask, np.random.choice(flattened_data, len(x)), x)\n",
    "    return x_corrupted\n",
    "\n",
    "def from_df2DataLoader(df, is_labelled=False, label_name=None, custom_batch_size = 32, custom_shuffle = True):\n",
    "    assert isinstance(df, pd.DataFrame), 'Input is not a dataframe, please transform it into a pd.DataFrame'\n",
    "\n",
    "    dataset = CustomDataset(df, is_labelled=is_labelled, label_name=label_name)\n",
    "    dataloader = DataLoader(dataset, batch_size=custom_batch_size, shuffle=custom_shuffle)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, is_labelled=False, label_name=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.is_labelled = is_labelled\n",
    "        self.label_name = label_name\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.dataframe.iloc[index]\n",
    "\n",
    "        # Decide the value of x based on is_labelled flag\n",
    "        if self.is_labelled:\n",
    "            x = torch.tensor(row[self.dataframe.columns != self.label_name].values, dtype=torch.float32)\n",
    "        else:\n",
    "            x = torch.tensor(row.values, dtype=torch.float32)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the three elements of the architecture:\n",
    "# 1. Encoder (bringing corrupted samples to a latent space Z)\n",
    "# 2. Mask Estimator (determines which features of x have been replaced)\n",
    "# 3. Feature Estimator (predicts original x from corrupted ^x^)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super().__init__() # ensures the underlying initilization of the nn.Module superclass\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class MaskEstimator(nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim):\n",
    "        super(MaskEstimator, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class FeatureEstimator(nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim):\n",
    "        super(FeatureEstimator, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Prepare data\n",
    "# Remember, for this stage we will only work the UNLABELLED DATA\n",
    "## 0.1 Train-test(-validation) split\n",
    "ssl_data = df_phase2_unlabelled.copy()\n",
    "\n",
    "train_data, aux_data = train_test_split(ssl_data, test_size=0.3)  # 70% training, 30% aux\n",
    "valid_data, test_data = train_test_split(aux_data, test_size=0.5)  # 15% validation, 15% test\n",
    "\n",
    "## 0.2 Preprocess the data\n",
    "preprocessor = build_preprocessor_phase2(train_data)\n",
    "\n",
    "X_train_processed = preprocessor.fit_transform(train_data)\n",
    "X_test_processed = preprocessor.transform(test_data)\n",
    "X_valid_processed = preprocessor.transform(valid_data)\n",
    "\n",
    "# 0.3 Adapt the datasets to the Pytorch DataLoader format\n",
    "train_dataloader = from_df2DataLoader(pd.DataFrame(X_train_processed, columns = (get_feature_names_OneHotEncoder_preprocessor(preprocessor))))\n",
    "test_dataloader = from_df2DataLoader(pd.DataFrame(X_test_processed, columns = (get_feature_names_OneHotEncoder_preprocessor(preprocessor))))\n",
    "valid_dataloader = from_df2DataLoader(pd.DataFrame(X_valid_processed, columns = (get_feature_names_OneHotEncoder_preprocessor(preprocessor))))\n",
    "\n",
    "# 0.4 For consistency with posterior training loop\n",
    "train_array = np.vstack([batch.numpy() for batch in train_dataloader])\n",
    "test_array = np.vstack([batch.numpy() for batch in test_dataloader])\n",
    "valid_array = np.vstack([batch.numpy() for batch in valid_dataloader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train_processed.shape[1]\n",
    "latent_dim = 5\n",
    "\n",
    "encoder = Encoder(input_dim, latent_dim)\n",
    "mask_estimator = MaskEstimator(latent_dim, input_dim)\n",
    "feature_estimator = FeatureEstimator(latent_dim, input_dim)\n",
    "\n",
    "optimizer = optim.Adam(list(encoder.parameters()) + list(mask_estimator.parameters()) + list(feature_estimator.parameters()), lr=0.001)\n",
    "loss_fn_mask = nn.BCELoss()\n",
    "loss_fn_feature = nn.MSELoss()\n",
    "\n",
    "num_epochs = 100\n",
    "alpha = 0.5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training loop\n",
    "    for batch_data in train_dataloader:\n",
    "        # Mask and corrupt data\n",
    "        mask = generate_mask_vector(input_dim, pm=0.5)\n",
    "        x_corrupted = generate_masked_sample(batch_data, mask, train_array)\n",
    "        \n",
    "        # Forward pass\n",
    "        z = encoder(x_corrupted)\n",
    "        mask_pred = mask_estimator(z)\n",
    "        feature_pred = feature_estimator(z)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss_mask = loss_fn_mask(mask_pred, mask)\n",
    "        loss_feature = loss_fn_feature(feature_pred, batch_data)\n",
    "        loss = loss_mask + alpha * loss_feature\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Testing loop: \n",
    "    # torch.no_grad() to ensure we are NOT updating the model weights outside the training loop\n",
    "    with torch.no_grad():\n",
    "        total_test_loss = 0\n",
    "        for batch_data in test_dataloader:\n",
    "            # Mask and corrupt data\n",
    "            mask = generate_mask_vector(input_dim, pm=0.5)\n",
    "            x_corrupted = generate_masked_sample(batch_data, mask, test_array)\n",
    "            \n",
    "            # Forward pass\n",
    "            z = encoder(x_corrupted)\n",
    "            mask_pred = mask_estimator(z)\n",
    "            feature_pred = feature_estimator(z)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss_mask = loss_fn_mask(mask_pred, mask)\n",
    "            loss_feature = loss_fn_feature(feature_pred, batch_data)\n",
    "            loss = loss_mask + alpha * loss_feature\n",
    "            \n",
    "            total_test_loss += loss.item()\n",
    "        \n",
    "        # Print or store validation loss\n",
    "        avg_test_loss = total_test_loss / len(test_data)\n",
    "        print(f\"Epoch {epoch+1}, Test Loss: {avg_test_loss}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
