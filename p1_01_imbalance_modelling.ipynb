{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (phase 1) 01 Class imbalance modelling\n",
    "\n",
    "The population of phase 1 presents a high level of overlap, thus in this notebook the textbook approaches for class overlap are implemented:\n",
    "- Resampling techniques (4 different resampling techniques)\n",
    "- Various model classes: logistic regression, random forest and XGBoost\n",
    "- Cost sensitive learning and moving threshold is applied in every case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages and load the data for phase 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, KFold\n",
    "from sklearn.metrics import f1_score, make_scorer, confusion_matrix, recall_score, roc_auc_score, roc_curve, average_precision_score\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import importlib\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# import from custom package\n",
    "from auxFuns.EDA import *\n",
    "from auxFuns.modelling import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'auxFuns.EDA' from 'c:\\\\Users\\\\angel\\\\Documents\\\\VSCode\\\\rsv_modelling_transfer_learning\\\\auxFuns\\\\EDA.py'>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import auxFuns.modelling \n",
    "importlib.reload(auxFuns.modelling)\n",
    "\n",
    "import auxFuns.EDA \n",
    "importlib.reload(auxFuns.EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load phase 1 data\n",
    "raw_datasets_path = os.getcwd() + '/datasets/raw'\n",
    "processed_datasets_path = os.getcwd() + '/datasets/processed'\n",
    "\n",
    "# Phase 1 data\n",
    "rsv_predictors_df_v2 = pd.read_csv(processed_datasets_path + '/rsv_predictors_phase1_daysDedup_seasons_prevTest_v2.csv',low_memory=False)\n",
    "rsv_predictors_phase1_df = make_it_categorical_v2(rsv_predictors_df_v2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86058, 21)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Following the model selection step, these are the final features taken for modelling\n",
    "selected_features_v2 = ['n_tests_that_day', 'sine','cosine', 'previous_test_daydiff',\n",
    "                     'Bronchitis', 'CCI',\n",
    "                     'Acute_upper_respiratory_infection', 'n_immunodeficiencies', 'n_symptoms',\n",
    "                     'healthcare_seeking', \n",
    "                     'General_symptoms_and_signs', 'prev_positive_rsv', 'Influenza',\n",
    "                     'key_comorbidities','Pneumonia',\n",
    "                     'season','month_of_the_test','multiple_tests',\n",
    "                     'BPA','BPAI']\n",
    "selected_features_v2.append('RSV_test_result')\n",
    "\n",
    "df_modelling_phase1 = rsv_predictors_phase1_df[selected_features_v2]\n",
    "\n",
    "df_modelling_phase1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Resampling of the data\n",
    "\n",
    "Four different techniques: \n",
    "- Random Undersampling: This involves removing some of the majority class instances. While it can help balance the classes, it may also remove important information, thereby affecting the model's ability to generalize.\n",
    "- Random Oversampling: This technique involves duplicating some minority class instances. Although this can balance the classes, it may lead to overfitting as the same instances are repeated.\n",
    "- SMOTE-NC, i.e. SMOTE (Synthetic Minority Oversampling Technique) adapted for mixed data (continuous and categorical variables). It generates synthetic instances of the minority class. While it adds diversity and avoids overfitting to some extent, it can also introduce noise.\n",
    "- Undersampling and Upweighting: This approach combines undersampling with the assignment of greater importance to the minority class. By upweighting the remaining majority instances, the model learns the imbalance of the actual data. It attempts to find a balance but may still risk losing information from the majority class or introducing bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----\n",
      "Resampling using None\n",
      "Resampling method chosen:\n",
      "\n",
      "None\n",
      "y_TRAIN imbalance ratio: 31.428638718794158\n",
      "y_TEST imbalance ratio: 31.41431261770245\n",
      "\n",
      "----\n",
      "Resampling using over\n",
      "Resampling method chosen:\n",
      "\n",
      "Oversampling\n",
      "y_TRAIN imbalance ratio: 1.2500093671550077\n",
      "y_TEST imbalance ratio: 31.41431261770245\n",
      "\n",
      "----\n",
      "Resampling using under\n",
      "Resampling method chosen:\n",
      "\n",
      "Undersampling\n",
      "y_TRAIN imbalance ratio: 1.249646726330664\n",
      "y_TEST imbalance ratio: 31.41431261770245\n",
      "\n",
      "----\n",
      "Resampling using smotenc\n",
      "Resampling method chosen:\n",
      "\n",
      "SMOTE-sampling\n",
      "y_TRAIN imbalance ratio: 1.2500093671550077\n",
      "y_TEST imbalance ratio: 31.41431261770245\n",
      "\n",
      "----\n",
      "Resampling using downsample_upweight\n",
      "Resampling method chosen:\n",
      "\n",
      "Downsampling and Upweighting\n",
      "y_TRAIN imbalance ratio: 0.7998115873763542\n",
      "y_TEST imbalance ratio: 31.41431261770245\n"
     ]
    }
   ],
   "source": [
    "resampling_techniques = ['None', 'over', 'under', 'smotenc', 'downsample_upweight']\n",
    "input_test_size = 0.2\n",
    "random_seed = 42\n",
    "\n",
    "resampled_data = {'None': {},\n",
    "                  'over': {},\n",
    "                  'under': {},\n",
    "                  'smotenc': {},\n",
    "                  'downsample_upweight':{}}\n",
    "\n",
    "for r in resampling_techniques:\n",
    "    print('\\n----')\n",
    "    print(f'Resampling using {r}')\n",
    "\n",
    "    X_train, y_train, X_test, y_test, sample_weights, preprocessor_rsv = preprocess_and_resample_rsv(\n",
    "        df_modelling_phase1, input_test_size = input_test_size, random_seed = random_seed, resampling_technique = r)\n",
    "    \n",
    "    resampled_data[r]['X_train'] = X_train\n",
    "    resampled_data[r]['y_train'] = y_train\n",
    "    resampled_data[r]['X_test'] = X_test\n",
    "    resampled_data[r]['y_test'] = y_test\n",
    "    resampled_data[r]['sample_weights'] = sample_weights\n",
    "    resampled_data[r]['preprocessor_rsv'] = preprocessor_rsv\n",
    "    \n",
    "    IR_train = y_train.value_counts()['Negative'] / y_train.value_counts()['Positive']\n",
    "    IR_test = y_test.value_counts()['Negative'] / y_test.value_counts()['Positive']\n",
    "\n",
    "    print(f'y_TRAIN imbalance ratio: {IR_train}')\n",
    "    print(f'y_TEST imbalance ratio: {IR_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fitting of all of ML models and performance evaluaton\n",
    "- All with cost senstive learning\n",
    "- All with moving threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------\n",
      "Model fitted using Logistic regression and resampling: None\n",
      "Training model ... LogisticRegression(class_weight={'Negative': 1, 'Positive': 10},\n",
      "                   random_state=42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\angel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\angel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\angel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\angel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\angel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\angel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\angel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\angel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\angel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\angel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\angel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\angel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\angel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\angel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\angel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\angel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\angel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\angel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\angel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\angel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\angel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\angel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\angel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\angel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\angel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best training parameters:  {'C': 0.01, 'max_iter': 20, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Best training f1-score:  0.33188139088157315\n",
      "Optimal threshold: 0.58\n",
      "Optimal f1: 0.36569987389659525\n",
      "\n",
      "\n",
      "AUC Score: 0.7912467594253123\n",
      "Precision / Positive predictive value: 0.5534351145038168\n",
      "Specificity: 0.9929860320124693\n",
      "Recall / sensitivity: 0.2730696798493409\n",
      "Negative predictive value: 0.9772271386430679\n",
      "Accuracy: 0.9707762026493144\n",
      "F-1: 0.36569987389659525\n",
      "Precision-Recall AUC: 0.34566211546801523\n",
      "\n",
      "-------------\n",
      "Model fitted using Logistic regression and resampling: over\n",
      "Training model ... LogisticRegression(class_weight={'Negative': 1, 'Positive': 10},\n",
      "                   random_state=42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\angel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best training parameters:  {'C': 10, 'max_iter': 20, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Best training f1-score:  0.6313159827750442\n",
      "Optimal threshold: 0.99\n",
      "Optimal f1: 0.3490136570561457\n",
      "\n",
      "\n",
      "AUC Score: 0.7891314034901736\n",
      "Precision / Positive predictive value: 0.8984375\n",
      "Specificity: 0.9992206702236077\n",
      "Recall / sensitivity: 0.21657250470809794\n",
      "Negative predictive value: 0.975649730742215\n",
      "Accuracy: 0.9750755287009063\n",
      "F-1: 0.3490136570561457\n",
      "Precision-Recall AUC: 0.3387452372786378\n",
      "\n",
      "-------------\n",
      "Model fitted using Logistic regression and resampling: under\n",
      "Training model ... LogisticRegression(class_weight={'Negative': 1, 'Positive': 10},\n",
      "                   random_state=42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\angel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\angel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\angel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best training parameters:  {'C': 1, 'max_iter': 20, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Best training f1-score:  0.6312970449812979\n",
      "Optimal threshold: 0.99\n",
      "Optimal f1: 0.3538461538461539\n",
      "\n",
      "\n",
      "AUC Score: 0.788229241496381\n",
      "Precision / Positive predictive value: 0.9663865546218487\n",
      "Specificity: 0.9997602062226485\n",
      "Recall / sensitivity: 0.21657250470809794\n",
      "Negative predictive value: 0.9756625519218394\n",
      "Accuracy: 0.9755984197071811\n",
      "F-1: 0.3538461538461539\n",
      "Precision-Recall AUC: 0.336536343481789\n",
      "\n",
      "-------------\n",
      "Model fitted using Logistic regression and resampling: smotenc\n",
      "Training model ... LogisticRegression(class_weight={'Negative': 1, 'Positive': 10},\n",
      "                   random_state=42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\angel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\angel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\angel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\angel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\angel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\angel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\angel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\angel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\angel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\angel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\angel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\angel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\angel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\angel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\angel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\angel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\angel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\angel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best training parameters:  {'C': 10, 'max_iter': 50, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Best training f1-score:  0.6580577533397802\n",
      "Optimal threshold: 0.99\n",
      "Optimal f1: 0.35082458770614694\n",
      "\n",
      "\n",
      "AUC Score: 0.7873677225156985\n",
      "Precision / Positive predictive value: 0.8602941176470589\n",
      "Specificity: 0.9988609795575805\n",
      "Recall / sensitivity: 0.22033898305084745\n",
      "Negative predictive value: 0.9757554462403373\n",
      "Accuracy: 0.9748431326981176\n",
      "F-1: 0.35082458770614694\n",
      "Precision-Recall AUC: 0.3423451242729777\n",
      "\n",
      "-------------\n",
      "Model fitted using Logistic regression and resampling: downsample_upweight\n",
      "Training model ... LogisticRegression(class_weight={'Negative': 1, 'Positive': 10},\n",
      "                   random_state=42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\angel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best training parameters:  {'C': 10, 'max_iter': 20, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Best training f1-score:  0.7217919072810398\n",
      "Optimal threshold: 0.99\n",
      "Optimal f1: 0.33568406205923834\n",
      "\n",
      "\n",
      "AUC Score: 0.7858928327288249\n",
      "Precision / Positive predictive value: 0.6685393258426966\n",
      "Specificity: 0.9964630417840658\n",
      "Recall / sensitivity: 0.224105461393597\n",
      "Negative predictive value: 0.9758130797229071\n",
      "Accuracy: 0.9726353706716244\n",
      "F-1: 0.33568406205923834\n",
      "Precision-Recall AUC: 0.3299868333571074\n"
     ]
    }
   ],
   "source": [
    "model_class = LogisticRegression(random_state= random_seed, \n",
    "                                class_weight= {'Negative':1, 'Positive': 10})\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'max_iter': [20, 50],\n",
    "    'solver':['liblinear']\n",
    "}\n",
    "\n",
    "target_scorer = make_scorer(f1_score, average='binary', pos_label = 'Positive')\n",
    "n_cv_folds = 5\n",
    "\n",
    "\n",
    "for r in resampling_techniques:\n",
    "    print('\\n-------------')\n",
    "    print(f'Model fitted using Logistic regression and resampling: {r}')\n",
    "\n",
    "    X_train = resampled_data[r]['X_train']\n",
    "    y_train = resampled_data[r]['y_train']\n",
    "    X_test = resampled_data[r]['X_test']\n",
    "    y_test = resampled_data[r]['y_test']\n",
    "    sample_weights = resampled_data[r]['sample_weights'] \n",
    "\n",
    "    model1 = train_model_rsv(model = model_class, param_grid = param_grid, target_scorer = target_scorer, n_cv_folds = n_cv_folds,\n",
    "                        X_train = X_train, y_train = y_train)\n",
    "    optimal_threshold = find_optimal_moving_threshold(trained_model = model1, X_test = X_test, y_test = y_test)\n",
    "\n",
    "\n",
    "    __,__,__,__,__,__,__,__ = calculate_performance_metrics_rsv(trained_model = model1, X_test = X_test, y_test = y_test,\n",
    "                                                            threshold = optimal_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.2. Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------\n",
      "Model fitted using Logistic regression and resampling: None\n",
      "Training model ... RandomForestClassifier(class_weight={'Negative': 1, 'Positive': 10},\n",
      "                       random_state=42)\n",
      "Best training parameters:  {'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 5, 'n_estimators': 14}\n",
      "Best training f1-score:  0.36654162066416374\n",
      "Optimal threshold: 0.46\n",
      "Optimal f1: 0.35469107551487417\n",
      "\n",
      "\n",
      "AUC Score: 0.789289516100899\n",
      "Precision / Positive predictive value: 0.4518950437317784\n",
      "Specificity: 0.9887296924644805\n",
      "Recall / sensitivity: 0.2919020715630885\n",
      "Negative predictive value: 0.9777105933961705\n",
      "Accuracy: 0.967232163606786\n",
      "F-1: 0.35469107551487417\n",
      "Precision-Recall AUC: 0.3366943872200664\n",
      "\n",
      "-------------\n",
      "Model fitted using Logistic regression and resampling: over\n",
      "Training model ... RandomForestClassifier(class_weight={'Negative': 1, 'Positive': 10},\n",
      "                       random_state=42)\n",
      "Best training parameters:  {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 7}\n",
      "Best training f1-score:  0.8108099431196788\n",
      "Optimal threshold: 0.99\n",
      "Optimal f1: 0.30744849445324884\n",
      "\n",
      "\n",
      "AUC Score: 0.7087510955267734\n",
      "Precision / Positive predictive value: 0.97\n",
      "Specificity: 0.9998201546669864\n",
      "Recall / sensitivity: 0.18267419962335216\n",
      "Negative predictive value: 0.9746376811594203\n",
      "Accuracy: 0.9746107366953288\n",
      "F-1: 0.30744849445324884\n",
      "Precision-Recall AUC: 0.2584638194657185\n",
      "\n",
      "-------------\n",
      "Model fitted using Logistic regression and resampling: under\n",
      "Training model ... RandomForestClassifier(class_weight={'Negative': 1, 'Positive': 10},\n",
      "                       random_state=42)\n",
      "Best training parameters:  {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 14}\n",
      "Best training f1-score:  0.6657421235754014\n",
      "Optimal threshold: 0.97\n",
      "Optimal f1: 0.32301480484522205\n",
      "\n",
      "\n",
      "AUC Score: 0.761943598561734\n",
      "Precision / Positive predictive value: 0.5660377358490566\n",
      "Specificity: 0.9944847431209161\n",
      "Recall / sensitivity: 0.22598870056497175\n",
      "Negative predictive value: 0.9758235294117648\n",
      "Accuracy: 0.9707762026493144\n",
      "F-1: 0.32301480484522205\n",
      "Precision-Recall AUC: 0.2920469566836317\n",
      "\n",
      "-------------\n",
      "Model fitted using Logistic regression and resampling: smotenc\n",
      "Training model ... RandomForestClassifier(class_weight={'Negative': 1, 'Positive': 10},\n",
      "                       random_state=42)\n",
      "Best training parameters:  {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 7}\n",
      "Best training f1-score:  0.8179492629896611\n",
      "Optimal threshold: 0.96\n",
      "Optimal f1: 0.28923076923076924\n",
      "\n",
      "\n",
      "AUC Score: 0.7408015547307281\n",
      "Precision / Positive predictive value: 0.7899159663865546\n",
      "Specificity: 0.9985012888915533\n",
      "Recall / sensitivity: 0.17702448210922786\n",
      "Negative predictive value: 0.9744339788217399\n",
      "Accuracy: 0.9731582616778991\n",
      "F-1: 0.28923076923076924\n",
      "Precision-Recall AUC: 0.2570289229054839\n",
      "\n",
      "-------------\n",
      "Model fitted using Logistic regression and resampling: downsample_upweight\n",
      "Training model ... RandomForestClassifier(class_weight={'Negative': 1, 'Positive': 10},\n",
      "                       random_state=42)\n",
      "Best training parameters:  {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 14}\n",
      "Best training f1-score:  0.7446843197940595\n",
      "Optimal threshold: 0.97\n",
      "Optimal f1: 0.3154034229828851\n",
      "\n",
      "\n",
      "AUC Score: 0.7613939018094156\n",
      "Precision / Positive predictive value: 0.44947735191637633\n",
      "Specificity: 0.9905281457946167\n",
      "Recall / sensitivity: 0.24293785310734464\n",
      "Negative predictive value: 0.9762481536189069\n",
      "Accuracy: 0.9674645596095747\n",
      "F-1: 0.3154034229828851\n",
      "Precision-Recall AUC: 0.283160817338798\n"
     ]
    }
   ],
   "source": [
    "cost_sensitive = True\n",
    "if cost_sensitive:\n",
    "    weight_dict = {'Negative': 1, 'Positive': 10}\n",
    "    model_class = RandomForestClassifier(class_weight= weight_dict, random_state= random_seed)\n",
    "else:\n",
    "    model_class = RandomForestClassifier(class_weight= None, random_state= random_seed)\n",
    "    \n",
    "param_grid = {\n",
    "    'n_estimators': [7, 14],\n",
    "    'max_depth': [10, 20],\n",
    "    'min_samples_split': [5, 10],\n",
    "    'min_samples_leaf': [1, 4]\n",
    "}\n",
    "\n",
    "\n",
    "target_scorer = make_scorer(f1_score, average='binary', pos_label = 'Positive')\n",
    "n_cv_folds = 5\n",
    "\n",
    "\n",
    "for r in resampling_techniques:\n",
    "    print('\\n-------------')\n",
    "    print(f'Model fitted using Random Forest and resampling: {r}')\n",
    "\n",
    "    X_train = resampled_data[r]['X_train']\n",
    "    y_train = resampled_data[r]['y_train']\n",
    "    X_test = resampled_data[r]['X_test']\n",
    "    y_test = resampled_data[r]['y_test']\n",
    "    sample_weights = resampled_data[r]['sample_weights'] \n",
    "\n",
    "    model1 = train_model_rsv(model = model_class, param_grid = param_grid, target_scorer = target_scorer, n_cv_folds = n_cv_folds,\n",
    "                        X_train = X_train, y_train = y_train, sample_weights = sample_weights)\n",
    "    optimal_threshold = find_optimal_moving_threshold(trained_model = model1, X_test = X_test, y_test = y_test)\n",
    "\n",
    "\n",
    "    __,__,__,__,__,__,__,__ = calculate_performance_metrics_rsv(trained_model = model1, X_test = X_test, y_test = y_test,\n",
    "                                                            threshold = optimal_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. LightGBM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------\n",
      "Model fitted using LightGBM and resampling: None\n",
      "Training model ... LGBMClassifier(class_weight={'Negative': 1, 'Positive': 10}, objective='binary',\n",
      "               random_state=42)\n",
      "Best training parameters:  {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 15, 'subsample': 0.7}\n",
      "Best training f1-score:  0.37566572907957885\n",
      "Optimal threshold: 0.45\n",
      "Optimal f1: 0.35857805255023184\n",
      "\n",
      "\n",
      "AUC Score: 0.7882138874692058\n",
      "Precision / Positive predictive value: 1.0\n",
      "Specificity: 1.0\n",
      "Recall / sensitivity: 0.2184557438794727\n",
      "Negative predictive value: 0.9757253158633599\n",
      "Accuracy: 0.975888914710667\n",
      "F-1: 0.35857805255023184\n",
      "Precision-Recall AUC: 0.34406210133077086\n",
      "\n",
      "-------------\n",
      "Model fitted using LightGBM and resampling: over\n",
      "Training model ... LGBMClassifier(class_weight={'Negative': 1, 'Positive': 10}, objective='binary',\n",
      "               random_state=42)\n",
      "Best training parameters:  {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 25, 'subsample': 0.7}\n",
      "Best training f1-score:  0.6652001604756863\n",
      "Optimal threshold: 0.97\n",
      "Optimal f1: 0.35348837209302325\n",
      "\n",
      "\n",
      "AUC Score: 0.7908652231397382\n",
      "Precision / Positive predictive value: 1.0\n",
      "Specificity: 1.0\n",
      "Recall / sensitivity: 0.21468926553672316\n",
      "Negative predictive value: 0.9756111825944555\n",
      "Accuracy: 0.9757727167092726\n",
      "F-1: 0.35348837209302325\n",
      "Precision-Recall AUC: 0.34134397853713566\n",
      "\n",
      "-------------\n",
      "Model fitted using LightGBM and resampling: under\n",
      "Training model ... LGBMClassifier(class_weight={'Negative': 1, 'Positive': 10}, objective='binary',\n",
      "               random_state=42)\n",
      "Best training parameters:  {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 25, 'subsample': 0.7}\n",
      "Best training f1-score:  0.648321486982616\n",
      "Optimal threshold: 0.96\n",
      "Optimal f1: 0.36229749631811486\n",
      "\n",
      "\n",
      "AUC Score: 0.792383521922559\n",
      "Precision / Positive predictive value: 0.831081081081081\n",
      "Specificity: 0.9985012888915533\n",
      "Recall / sensitivity: 0.23163841807909605\n",
      "Negative predictive value: 0.9760900140646976\n",
      "Accuracy: 0.9748431326981176\n",
      "F-1: 0.36229749631811486\n",
      "Precision-Recall AUC: 0.35060502685559247\n",
      "\n",
      "-------------\n",
      "Model fitted using LightGBM and resampling: smotenc\n",
      "Training model ... LGBMClassifier(class_weight={'Negative': 1, 'Positive': 10}, objective='binary',\n",
      "               random_state=42)\n",
      "Best training parameters:  {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 25, 'subsample': 0.7}\n",
      "Best training f1-score:  0.7284554857523396\n",
      "Optimal threshold: 0.9500000000000001\n",
      "Optimal f1: 0.35348837209302325\n",
      "\n",
      "\n",
      "AUC Score: 0.7783617952967228\n",
      "Precision / Positive predictive value: 1.0\n",
      "Specificity: 1.0\n",
      "Recall / sensitivity: 0.21468926553672316\n",
      "Negative predictive value: 0.9756111825944555\n",
      "Accuracy: 0.9757727167092726\n",
      "F-1: 0.35348837209302325\n",
      "Precision-Recall AUC: 0.3306582795748386\n",
      "\n",
      "-------------\n",
      "Model fitted using LightGBM and resampling: downsample_upweight\n",
      "Training model ... LGBMClassifier(class_weight={'Negative': 1, 'Positive': 10}, objective='binary',\n",
      "               random_state=42)\n",
      "Best training parameters:  {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 25, 'subsample': 0.7}\n",
      "Best training f1-score:  0.7349984288054003\n",
      "Optimal threshold: 0.98\n",
      "Optimal f1: 0.36529680365296807\n",
      "\n",
      "\n",
      "AUC Score: 0.7908028474043396\n",
      "Precision / Positive predictive value: 0.9523809523809523\n",
      "Specificity: 0.9996403093339727\n",
      "Recall / sensitivity: 0.22598870056497175\n",
      "Negative predictive value: 0.9759452183073861\n",
      "Accuracy: 0.9757727167092726\n",
      "F-1: 0.36529680365296807\n",
      "Precision-Recall AUC: 0.34490568638406865\n"
     ]
    }
   ],
   "source": [
    "cost_sensitive = True\n",
    "random_seed = 42  # As before, set your own seed\n",
    "\n",
    "if cost_sensitive:\n",
    "    weight_dict = {'Negative': 1, 'Positive': 10}\n",
    "    model_class = lgb.LGBMClassifier(class_weight=weight_dict, random_state=random_seed, objective='binary')\n",
    "else:\n",
    "    model_class = lgb.LGBMClassifier(random_state=random_seed, objective='binary')\n",
    "\n",
    "# LightGBM-specific parameters\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 15, 25],\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'subsample': [0.7, 0.9, 1]\n",
    "}\n",
    "\n",
    "target_scorer = make_scorer(f1_score, average='binary', pos_label='Positive')\n",
    "n_cv_folds = 5\n",
    "\n",
    "# Assuming resampled_data and other functions are defined elsewhere\n",
    "for r in resampling_techniques:\n",
    "    print('\\n-------------')\n",
    "    print(f'Model fitted using LightGBM and resampling: {r}')\n",
    "\n",
    "    X_train = resampled_data[r]['X_train']\n",
    "    y_train = resampled_data[r]['y_train']\n",
    "    X_test = resampled_data[r]['X_test']\n",
    "    y_test = resampled_data[r]['y_test']\n",
    "    sample_weights = resampled_data[r]['sample_weights']\n",
    "\n",
    "    model1 = train_model_rsv(model=model_class, param_grid=param_grid, target_scorer=target_scorer, n_cv_folds=n_cv_folds,\n",
    "                             X_train=X_train, y_train=y_train)\n",
    "    optimal_threshold = find_optimal_moving_threshold(trained_model=model1, X_test=X_test, y_test=y_test)\n",
    "\n",
    "    __, __, __, __, __, __, __, __ = calculate_performance_metrics_rsv(trained_model=model1, X_test=X_test, y_test=y_test,\n",
    "                                                                       threshold=optimal_threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------\n",
      "Model fitted using XGBoost and resampling: None\n",
      "Training model ... XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=42, ...)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[93], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m y_test \u001b[39m=\u001b[39m [\u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m label\u001b[39m==\u001b[39m\u001b[39m'\u001b[39m\u001b[39mPositive\u001b[39m\u001b[39m'\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m \u001b[39mfor\u001b[39;00m label \u001b[39min\u001b[39;00m resampled_data[r][\u001b[39m'\u001b[39m\u001b[39my_test\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[0;32m     30\u001b[0m sample_weights \u001b[39m=\u001b[39m resampled_data[r][\u001b[39m'\u001b[39m\u001b[39msample_weights\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m---> 32\u001b[0m model1 \u001b[39m=\u001b[39m train_model_rsv(model\u001b[39m=\u001b[39;49mmodel_class, param_grid\u001b[39m=\u001b[39;49mparam_grid, target_scorer\u001b[39m=\u001b[39;49mtarget_scorer, n_cv_folds\u001b[39m=\u001b[39;49mn_cv_folds,\n\u001b[0;32m     33\u001b[0m                          X_train\u001b[39m=\u001b[39;49mX_train, y_train\u001b[39m=\u001b[39;49my_train, sample_weights \u001b[39m=\u001b[39;49m sample_weights)\n\u001b[0;32m     34\u001b[0m optimal_threshold \u001b[39m=\u001b[39m find_optimal_moving_threshold(trained_model\u001b[39m=\u001b[39mmodel1, X_test\u001b[39m=\u001b[39mX_test, y_test\u001b[39m=\u001b[39my_test)\n\u001b[0;32m     36\u001b[0m __, __, __, __, __, __, __, __ \u001b[39m=\u001b[39m calculate_performance_metrics_rsv(trained_model\u001b[39m=\u001b[39mmodel1, X_test\u001b[39m=\u001b[39mX_test, y_test\u001b[39m=\u001b[39my_test,\n\u001b[0;32m     37\u001b[0m                                                                    threshold\u001b[39m=\u001b[39moptimal_threshold)\n",
      "File \u001b[1;32mc:\\Users\\angel\\Documents\\VSCode\\rsv_modelling_transfer_learning\\auxFuns\\modelling.py:522\u001b[0m, in \u001b[0;36mtrain_model_rsv\u001b[1;34m(model, param_grid, target_scorer, n_cv_folds, X_train, y_train, sample_weights)\u001b[0m\n\u001b[0;32m    518\u001b[0m grid_search \u001b[39m=\u001b[39m GridSearchCV(estimator\u001b[39m=\u001b[39mmodel, param_grid\u001b[39m=\u001b[39mparam_grid, scoring\u001b[39m=\u001b[39mtarget_scorer, cv\u001b[39m=\u001b[39mn_cv_folds)\n\u001b[0;32m    520\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTraining model ... \u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 522\u001b[0m grid_search\u001b[39m.\u001b[39;49mfit(X_train, y_train, sample_weight \u001b[39m=\u001b[39;49m sample_weights)\n\u001b[0;32m    524\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mBest training parameters: \u001b[39m\u001b[39m'\u001b[39m, grid_search\u001b[39m.\u001b[39mbest_params_)\n\u001b[0;32m    525\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mBest training f1-score: \u001b[39m\u001b[39m'\u001b[39m, grid_search\u001b[39m.\u001b[39mbest_score_)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    868\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    870\u001b[0m     )\n\u001b[0;32m    872\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 874\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    876\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    878\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1386\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1387\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    814\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    815\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[0;32m    818\u001b[0m         )\n\u001b[0;32m    819\u001b[0m     )\n\u001b[1;32m--> 821\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    822\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    823\u001b[0m         clone(base_estimator),\n\u001b[0;32m    824\u001b[0m         X,\n\u001b[0;32m    825\u001b[0m         y,\n\u001b[0;32m    826\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[0;32m    827\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[0;32m    828\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[0;32m    829\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[0;32m    830\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[0;32m    831\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[0;32m    832\u001b[0m     )\n\u001b[0;32m    833\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[0;32m    834\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[0;32m    835\u001b[0m     )\n\u001b[0;32m    836\u001b[0m )\n\u001b[0;32m    838\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    839\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    843\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1089\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    684\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    685\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 686\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, y_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    688\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    689\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    690\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\xgboost\\core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[0;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[1;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\xgboost\\sklearn.py:1490\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1462\u001b[0m (\n\u001b[0;32m   1463\u001b[0m     model,\n\u001b[0;32m   1464\u001b[0m     metric,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1469\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[0;32m   1470\u001b[0m )\n\u001b[0;32m   1471\u001b[0m train_dmatrix, evals \u001b[39m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[0;32m   1472\u001b[0m     missing\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmissing,\n\u001b[0;32m   1473\u001b[0m     X\u001b[39m=\u001b[39mX,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1487\u001b[0m     feature_types\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_types,\n\u001b[0;32m   1488\u001b[0m )\n\u001b[1;32m-> 1490\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(\n\u001b[0;32m   1491\u001b[0m     params,\n\u001b[0;32m   1492\u001b[0m     train_dmatrix,\n\u001b[0;32m   1493\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_num_boosting_rounds(),\n\u001b[0;32m   1494\u001b[0m     evals\u001b[39m=\u001b[39;49mevals,\n\u001b[0;32m   1495\u001b[0m     early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds,\n\u001b[0;32m   1496\u001b[0m     evals_result\u001b[39m=\u001b[39;49mevals_result,\n\u001b[0;32m   1497\u001b[0m     obj\u001b[39m=\u001b[39;49mobj,\n\u001b[0;32m   1498\u001b[0m     custom_metric\u001b[39m=\u001b[39;49mmetric,\n\u001b[0;32m   1499\u001b[0m     verbose_eval\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m   1500\u001b[0m     xgb_model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m   1501\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m   1502\u001b[0m )\n\u001b[0;32m   1504\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m callable(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective):\n\u001b[0;32m   1505\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective \u001b[39m=\u001b[39m params[\u001b[39m\"\u001b[39m\u001b[39mobjective\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\xgboost\\core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[0;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[1;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\xgboost\\training.py:185\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    184\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m bst\u001b[39m.\u001b[39;49mupdate(dtrain, i, obj)\n\u001b[0;32m    186\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    187\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\xgboost\\core.py:1918\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1915\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_dmatrix_features(dtrain)\n\u001b[0;32m   1917\u001b[0m \u001b[39mif\u001b[39;00m fobj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1918\u001b[0m     _check_call(_LIB\u001b[39m.\u001b[39;49mXGBoosterUpdateOneIter(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle,\n\u001b[0;32m   1919\u001b[0m                                             ctypes\u001b[39m.\u001b[39;49mc_int(iteration),\n\u001b[0;32m   1920\u001b[0m                                             dtrain\u001b[39m.\u001b[39;49mhandle))\n\u001b[0;32m   1921\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1922\u001b[0m     pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict(dtrain, output_margin\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cost_sensitive = True\n",
    "\n",
    "if cost_sensitive:\n",
    "    weight_scale = 10  \n",
    "    model_class = xgb.XGBClassifier(scale_pos_weight=weight_scale, random_state=random_seed, objective='binary:logistic')\n",
    "else:\n",
    "    model_class = xgb.XGBClassifier(random_state=random_seed, objective='binary:logistic')\n",
    "\n",
    "# XGBoost-specific parameters\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.7, 0.9, 1],\n",
    "    'colsample_bytree': [0.7, 0.9, 1]\n",
    "}\n",
    "\n",
    "target_scorer = make_scorer(f1_score, average='binary', pos_label= 1)\n",
    "n_cv_folds = 5\n",
    "\n",
    "# Assuming resampled_data and other functions are defined elsewhere\n",
    "for r in resampling_techniques:\n",
    "    print('\\n-------------')\n",
    "    print(f'Model fitted using XGBoost and resampling: {r}')\n",
    "\n",
    "    X_train = resampled_data[r]['X_train']\n",
    "    y_train = [1 if label=='Positive' else 0 for label in resampled_data[r]['y_train']] # needed for XGBoost to be fitted\n",
    "    X_test = resampled_data[r]['X_test']\n",
    "    y_test = [1 if label=='Positive' else 0 for label in resampled_data[r]['y_test']]\n",
    "    sample_weights = resampled_data[r]['sample_weights']\n",
    "\n",
    "    model1 = train_model_rsv(model=model_class, param_grid=param_grid, target_scorer=target_scorer, n_cv_folds=n_cv_folds,\n",
    "                             X_train=X_train, y_train=y_train, sample_weights = sample_weights)\n",
    "    optimal_threshold = find_optimal_moving_threshold(trained_model=model1, X_test=X_test, y_test=y_test)\n",
    "\n",
    "    __, __, __, __, __, __, __, __ = calculate_performance_metrics_rsv(trained_model=model1, X_test=X_test, y_test=y_test,\n",
    "                                                                       threshold=optimal_threshold)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
